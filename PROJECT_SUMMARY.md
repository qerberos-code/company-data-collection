# Company Data Collection Prototype - Project Summary

## ğŸ¯ Project Overview

This is a comprehensive **agentic AI prototype** for company data collection, preparation, and validation, specifically designed to collect information about **Alphabet Holdings** (Google's parent company) from Wikipedia and process it through a sophisticated pipeline.

## ğŸ—ï¸ Architecture

### Core Components

1. **Data Collection** (`src/collection/`)
   - Wikipedia data extraction
   - Company information parsing
   - Domain and acquisition discovery

2. **Data Preparation** (`src/preparation/`)
   - **4-Stage Pipeline**:
     - Stage 1: Data Entry (hierarchy with search terms)
     - Stage 2: Domain Association (domains + ASNs/Netblocks)
     - Stage 3: DANS Check (digital asset verification)
     - Stage 4: Enumeration (all name representations)

3. **Data Validation** (`src/validation/`)
   - **2-Stage Validation**:
     - Stage 1: Source validation (search terms â†’ digital assets)
     - Stage 2: Final validation (hierarchy completeness)

4. **Agentic AI** (`src/agents/`)
   - LangChain-powered orchestration
   - Intelligent tool selection
   - Automated pipeline execution

5. **Database Storage** (`src/database/`, `src/models/`)
   - PostgreSQL integration
   - Comprehensive schema design
   - JSON data storage

## ğŸ“Š Data Schema

### Core Tables
- **Companies**: Main entity with legal/colloquial names
- **Domains**: Domain names with ASN/Netblock info
- **Acquisitions**: Company acquisitions and mergers
- **Brands**: Company brands and products
- **Data Sources**: Source tracking and raw data
- **Processing Stages**: Pipeline stage monitoring
- **Validation Results**: Quality assurance results

## ğŸš€ Key Features

### Data Collection
- âœ… Wikipedia API integration
- âœ… Company hierarchy extraction
- âœ… Domain name discovery
- âœ… Acquisition tracking
- âœ… Brand identification

### Data Preparation
- âœ… Search term generation
- âœ… Domain analysis and verification
- âœ… ASN/Netblock discovery
- âœ… Name variation enumeration
- âœ… Cross-reference validation

### Data Validation
- âœ… Source verification
- âœ… Digital asset validation
- âœ… Hierarchy completeness
- âœ… Data consistency checks
- âœ… Quality scoring (1-100)

### Agentic AI
- âœ… LangChain integration
- âœ… OpenAI GPT-4 powered
- âœ… Tool-based architecture
- âœ… Automated orchestration
- âœ… Intelligent decision making

## ğŸ“ Project Structure

```
datacollection/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/           # Agentic AI components
â”‚   â”œâ”€â”€ collection/       # Data collection modules
â”‚   â”œâ”€â”€ preparation/      # Data preparation pipeline
â”‚   â”œâ”€â”€ validation/       # Data validation pipeline
â”‚   â”œâ”€â”€ database/         # Database connection
â”‚   â”œâ”€â”€ models/          # Database models
â”‚   â””â”€â”€ config/          # Configuration
â”œâ”€â”€ tests/               # Test suite
â”œâ”€â”€ logs/               # Log files
â”œâ”€â”€ data/               # Data storage
â”œâ”€â”€ main.py             # Main orchestration script
â”œâ”€â”€ example.py          # Usage examples
â”œâ”€â”€ requirements.txt    # Dependencies
â”œâ”€â”€ setup.py           # Package setup
â”œâ”€â”€ README.md          # Documentation
â””â”€â”€ config.env.example # Configuration template
```

## ğŸ› ï¸ Technology Stack

### Core Technologies
- **Python 3.11+**: Main programming language
- **LangChain**: Agentic AI framework
- **OpenAI GPT-4**: Language model
- **PostgreSQL**: Database storage
- **SQLAlchemy**: ORM
- **Pydantic**: Data validation

### Data Processing
- **Wikipedia API**: Data source
- **BeautifulSoup**: HTML parsing
- **Requests**: HTTP client
- **DNS Python**: Domain resolution
- **Pandas**: Data manipulation

### Development Tools
- **Pytest**: Testing framework
- **Black**: Code formatting
- **Flake8**: Linting
- **MyPy**: Type checking
- **Rich**: Terminal output
- **Loguru**: Logging

## ğŸ¯ Use Case: Alphabet Holdings

### Target Data Collection
- **Company Names**: Alphabet Inc., Google LLC
- **Legal Names**: Alphabet Inc., Google LLC
- **Colloquial Names**: Google, Alphabet
- **Domains**: google.com, alphabet.com, youtube.com, etc.
- **Acquisitions**: YouTube, Android, DeepMind, etc.
- **Brands**: Google, YouTube, Android, Chrome, Gmail, etc.
- **Subsidiaries**: Google LLC, YouTube LLC, Waymo, etc.

### Expected Output
```json
{
  "company": {
    "name": "Alphabet Inc.",
    "legal_name": "Alphabet Inc.",
    "colloquial_name": "Google"
  },
  "subsidiaries": ["Google LLC", "YouTube", "Waymo", "DeepMind"],
  "brands": ["Google", "YouTube", "Android", "Chrome", "Gmail"],
  "digital_assets": {
    "domains": [
      {"domain": "google.com", "is_active": true, "asn": "AS15169"},
      {"domain": "youtube.com", "is_active": true, "asn": "AS15169"}
    ],
    "asns": ["AS15169"],
    "netblocks": ["8.8.8.0/24", "142.250.0.0/16"]
  },
  "validation_summary": {
    "overall_score": 85.5,
    "validation_results": [
      {"type": "source", "status": "passed", "score": 88},
      {"type": "validation", "status": "passed", "score": 83}
    ]
  }
}
```

## ğŸš€ Quick Start

### 1. Setup Environment
```bash
# Create virtual environment
python3 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Configure Settings
```bash
# Copy configuration template
cp config.env.example .env

# Edit .env with your settings:
# - Database credentials
# - OpenAI API key
# - LangChain API key
```

### 3. Initialize Database
```bash
# Create PostgreSQL database
createdb company_data

# Initialize tables
python main.py --init-db
```

### 4. Run Collection
```bash
# Process Alphabet Inc. (default)
python main.py

# Process specific company
python main.py --company "Microsoft Corporation"

# Process multiple companies
python main.py --companies "Alphabet Inc." "Microsoft Corporation"
```

## ğŸ“ˆ Pipeline Flow

```
Wikipedia Data â†’ Collection â†’ Preparation â†’ Validation â†’ Database
     â†“              â†“            â†“            â†“           â†“
  Raw JSON â†’ Structured Data â†’ Enhanced Data â†’ Validated Data â†’ Stored Data
```

### Detailed Flow
1. **Collection**: Extract company data from Wikipedia
2. **Preparation Stage 1**: Generate search terms and hierarchy
3. **Preparation Stage 2**: Associate domains with logical parents
4. **Preparation Stage 3**: Check for additional digital assets
5. **Preparation Stage 4**: Enumerate all name representations
6. **Validation Stage 1**: Verify search terms against digital assets
7. **Validation Stage 2**: Complete hierarchy validation
8. **Storage**: Save to PostgreSQL with full audit trail

## ğŸ§ª Testing

```bash
# Run tests
pytest tests/

# Run with coverage
pytest --cov=src tests/

# Run specific test
pytest tests/test_pipeline.py::TestWikipediaCollector::test_collect_company_data_success
```

## ğŸ“Š Quality Metrics

### Validation Scoring
- **Source Validation**: 0-100 (search terms â†’ digital assets)
- **Final Validation**: 0-100 (hierarchy completeness)
- **Overall Score**: Average of all validation scores
- **Pass Threshold**: 70+ overall score

### Data Quality Indicators
- âœ… Search term coverage
- âœ… Domain verification
- âœ… ASN/Netblock validation
- âœ… Cross-reference consistency
- âœ… Hierarchy completeness

## ğŸ”§ Configuration Options

### Key Settings
```env
# Target Company
TARGET_COMPANY=Alphabet Inc.
COMPANY_SEARCH_TERMS=Alphabet,Google,Alphabet Holdings,Google LLC

# Database
DATABASE_URL=postgresql://user:pass@localhost:5432/company_data

# AI Services
OPENAI_API_KEY=your_openai_api_key
LANGCHAIN_API_KEY=your_langchain_api_key

# Processing
MAX_RETRIES=3
REQUEST_DELAY=1.0
BATCH_SIZE=10
```

## ğŸ‰ Success Criteria

### Functional Requirements
- âœ… Collect company data from Wikipedia
- âœ… Process through 4-stage preparation pipeline
- âœ… Validate through 2-stage validation process
- âœ… Store in PostgreSQL database
- âœ… Provide JSON output format
- âœ… Support agentic AI orchestration

### Quality Requirements
- âœ… Data accuracy > 80%
- âœ… Processing pipeline completion
- âœ… Validation score > 70%
- âœ… Database integrity
- âœ… Error handling and logging

## ğŸš€ Future Enhancements

### Potential Improvements
- **Additional Data Sources**: SEC filings, company websites, news articles
- **Enhanced AI**: More sophisticated reasoning and decision making
- **Real-time Updates**: Continuous data monitoring and updates
- **Advanced Analytics**: Trend analysis, competitive intelligence
- **API Interface**: REST API for external integrations
- **Web Dashboard**: User interface for data visualization

## ğŸ“ Conclusion

This prototype successfully demonstrates a comprehensive agentic AI system for company data collection, preparation, and validation. The system is specifically designed for Alphabet Holdings but can be easily adapted for other companies. The modular architecture, comprehensive testing, and detailed documentation make it a solid foundation for production deployment.

**Key Achievements:**
- âœ… Complete end-to-end pipeline
- âœ… Agentic AI orchestration
- âœ… Comprehensive data validation
- âœ… PostgreSQL integration
- âœ… JSON data format
- âœ… Extensive documentation
- âœ… Test coverage
- âœ… Production-ready architecture
